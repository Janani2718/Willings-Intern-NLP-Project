{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msmka\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# !pip install pdfminer.six\n",
    "# !pip install pdfminer3\n",
    "# !pip install pdfminer\n",
    "# !pip install pyMuPDF\n",
    "# !pip install -q wordcloud\n",
    "import wordcloud\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "import sys, fitz\n",
    "from pdf2image import convert_from_path\n",
    "import easyocr\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pytesseract\n",
    "import sklearn\n",
    "import joblib\n",
    "\n",
    "!pip install spacy\n",
    "import spacy\n",
    "spacy.prefer_gpu()\n",
    "!python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from spacy.matcher import Matcher\n",
    "!pip install -U spacy\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer,TfidfTransformer\n",
    "import re\n",
    "from pdfminer.high_level import extract_text\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "!pip install spacy\n",
    "import spacy\n",
    "spacy.prefer_gpu()\n",
    "!python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from spacy.matcher import Matcher\n",
    "from pdfminer3.layout import LAParams, LTTextBox\n",
    "from pdfminer3.pdfpage import PDFPage\n",
    "from pdfminer3.pdfinterp import PDFResourceManager,PDFPageInterpreter\n",
    "from pdfminer3.converter import PDFPageAggregator,TextConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GALKResumeParser(Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "   \n",
    "    def get_name(self):\n",
    "        nlp=spacy.load('output3/model-best')\n",
    "        doc=nlp(self.text)\n",
    "#         print(doc.ents)\n",
    "        try:\n",
    "            self.name=doc.ents[0]\n",
    "        except:\n",
    "            self.name=\"Name not found\"\n",
    "    def convertPDFtoTEXT(self,resume):\n",
    "        \"\"\"\n",
    "        Function to extract text using pdfminer3 from PDF file.\n",
    "        input:\n",
    "            resume_file: string\n",
    "            Tells the absolute path to the resume location.\n",
    "        output:\n",
    "            text: string\n",
    "            Raw text that has been extracted from the PDF file.\n",
    "        \"\"\"\n",
    "        resource_manager = PDFResourceManager()\n",
    "        file_handle = io.StringIO()\n",
    "        converter = TextConverter(resource_manager, file_handle, laparams=LAParams())\n",
    "        page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "\n",
    "        with open(resume, 'rb') as fh:\n",
    "            for page in PDFPage.get_pages(fh,\n",
    "                                          caching=True,\n",
    "                                          check_extractable=True):\n",
    "                page_interpreter.process_page(page)\n",
    "            text = file_handle.getvalue()\n",
    "        converter.close()\n",
    "        file_handle.close()\n",
    "        self.text=text\n",
    "\n",
    "    def ProcessConverted(self,text):\n",
    "        \"\"\"\n",
    "        Function that cleans the converted text and allows limited character types in it.\n",
    "        Input:\n",
    "            text: string\n",
    "            Raw text to be processed\n",
    "        Output:\n",
    "            sentences: list\n",
    "            List of text sentences after being processed \n",
    "        \"\"\"\n",
    "        sentences=[]\n",
    "        allowed=[\"/\",\"-\",\":\",\"(\",\")\",\",\",\" \",\".\"]\n",
    "        allowed.extend(list(string.ascii_letters+string.digits))\n",
    "        for s in text.split(\"\\n\"):\n",
    "            if(s==len(s)*\" \"):\n",
    "                continue\n",
    "            t=\"\"\n",
    "            for letter in s:\n",
    "                if letter in allowed:\n",
    "                    t+=letter\n",
    "            t=t.strip()\n",
    "            if(t):\n",
    "                sentences.append(t)\n",
    "        self.text=\"\\n\".join(sentences)\n",
    "    \n",
    "    def fit(self,resume_location):\n",
    "        self.resume=resume_location\n",
    "        self.convertPDFtoTEXT(self.resume)\n",
    "        self.ProcessConverted(self.text)\n",
    "        self.get_designation()\n",
    "        self.Organizations()\n",
    "        self.get_name()\n",
    "#         self.name=\"\"\n",
    "        self.get_skills()\n",
    "        self.spacy_ner()\n",
    "        self.classifier_model()\n",
    "    \n",
    "    def common_func(self,RESERVED_WORDS,lst):\n",
    "        organizations = []\n",
    "        for sent in nltk.sent_tokenize(self.text):\n",
    "            for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "                if hasattr(chunk, 'label'):\n",
    "                    organizations.append(' '.join(c[0] for c in chunk.leaves()))\n",
    "                elif chunk[1]=='IN':\n",
    "                    organizations.append(chunk[0])\n",
    "        a=[]\n",
    "        a.append(organizations[0])\n",
    "        for i in range(1,len(organizations)-1):\n",
    "            if organizations[i] in lst:\n",
    "                a.append(organizations[i-1]+' '+organizations[i]+' '+organizations[i+1])\n",
    "            else:\n",
    "                a.append(organizations[i])\n",
    "        a.append(organizations[len(organizations)-1])\n",
    "        \n",
    "        education = set()\n",
    "        for org in a:\n",
    "            for i in org.split():\n",
    "                if i.lower() in RESERVED_WORDS:\n",
    "                    education.add(org)\n",
    "        education=list(education)\n",
    "        final=set()\n",
    "        for i in education:\n",
    "            final.add(i)\n",
    "        return final\n",
    "    def Organizations(self):\n",
    "        IITS = ['IIT Madras','IIT Delhi','IIT Bombay','IIT Kanpur','IIT Kharagpur','IIT Roorkee','IIT Guwahati','IIT Hyderabad','IIT Dhanbad','IIT Indore','IIT Varanasi','IIT BHU','IIT Ropar','IIT Patna','IIT Gandhinagar','IIT Bhubaneswar','IIT Mandi','IIT Jodhpur','IIT Tirupati','IIT Bhilai','IIT Goa','IIT Jammu','IIT Dharwad','IIT Palakkad']\n",
    "        Indian_Institute = ['Indian Institute of Technology '+s[4:] for s in IITS]\n",
    "        Institution = IITS+Indian_Institute\n",
    "        EDUCATION = [\n",
    "            'BE','B.E.', 'B.E', 'BS', 'B.S', \n",
    "            'ME', 'M.E', 'M.E.', 'MS', 'M.S', \n",
    "            'BTECH', 'B.TECH', 'M.TECH', 'MTECH', \n",
    "             'B. Tech', 'M. Tech','SSC', 'HSC', 'CBSE', 'ICSE', 'X', 'XII'\n",
    "        ]\n",
    "        RESERVED_WORDS = [\n",
    "            \"class\",\n",
    "            \"institute of\",\n",
    "            \"b.tech\",\n",
    "            \"year\",\n",
    "            \"national institute\",\n",
    "            \"indian institute of technology\",\n",
    "            \"b.sc\",\n",
    "            \"IIT\",\n",
    "            \"technology\",\n",
    "            'school',\n",
    "            'college',\n",
    "            'university',\n",
    "            \"education\",\n",
    "            'academy',\n",
    "            'faculty',\n",
    "            'institute',\n",
    "            'faculdades',\n",
    "            'Schola',\n",
    "            'schule',\n",
    "            'lise',\n",
    "            'lyceum',\n",
    "            'lycee',\n",
    "            'polytechnic',\n",
    "            'kolej',\n",
    "            'Ã¼nivers',\n",
    "            'okul',\n",
    "            *EDUCATION,\n",
    "            *Institution\n",
    "        ]\n",
    "        \n",
    "        self.org=self.common_func(RESERVED_WORDS,[\"of\"])\n",
    "    \n",
    "    def get_designation(self):\n",
    "        RESERVED_WORDS = [\n",
    "            'assistant',\n",
    "            'associate',\n",
    "            'analyst',\n",
    "            'apprentice',\n",
    "            'intern',\n",
    "            'internship',\n",
    "            'manager',\n",
    "            'founder',\n",
    "            'cofounder',\n",
    "            'core',\n",
    "            'coordinator',\n",
    "            'volunteer',\n",
    "            'head',\n",
    "            'president',\n",
    "            'consultant',\n",
    "            'developer',\n",
    "            'lead'\n",
    "        ]\n",
    "        self.designation=self.common_func(RESERVED_WORDS,['of','at','in','for','to'])\n",
    "    def spacy_ner(self):\n",
    "        Skills = ['Flask','Django','Mysql','C','SQL','Css','html','js','Machine learning','C++','Algorithms','Github','Php','Python','opencv','deep learning','autocad','simulink','tensorflow','matlab','keras','numpy','nlp','data science','optimization','operational research',\n",
    "          'Natural Language Processing','probability','statistics','data analytics','Data Engineering','Data structures','sklearn','pandas','nltk','spacy']\n",
    "        Projects = ['Logistic Regression','Recommendation System','Pipeline','Decision Tree','Random Forest','Forecast','neural networks','pipeline','segmentation','data mining','neural network',\n",
    "             'Data acquisition','DATA ANALYSIS','FRAUD DETECTION','Knn classifier','Support Vector Machine']\n",
    "        Experience = ['Team Lead','Intern','Volunteer','ENGINEER','MACHINE LEARNING INTERN','DATA SCIENCE INTERN','SDE INTERN','BUSINESS DEVELOPMENT INTERN','FINANCE INTERN',\"Leader\",'Data Scientist']\n",
    "\n",
    "        IITS = ['IIT Madras','IIT Delhi','IIT Bombay','IIT Kanpur','IIT Kharagpur','IIT Roorkee','IIT Guwahati','IIT Hyderabad','IIT Dhanbad','IIT Indore','IIT Varanasi','IIT BHU','IIT Ropar','IIT Patna','IIT Gandhinagar','IIT Bhubaneswar','IIT Mandi','IIT Jodhpur','IIT Tirupati','IIT Bhilai','IIT Goa','IIT Jammu','IIT Dharwad','IIT Palakkad']\n",
    "        Indian_Institute = ['Indian Institute of Technology '+s[4:] for s in IITS]\n",
    "        Institution = IITS+Indian_Institute\n",
    "        EDUCATION = [\n",
    "            'BE','B.E.', 'B.E', 'BS', 'B.S', \n",
    "            'ME', 'M.E', 'M.E.', 'MS', 'M.S', \n",
    "            'BTECH', 'B.TECH', 'M.TECH', 'MTECH', \n",
    "             'B. Tech', 'M. Tech','SSC', 'HSC', 'CBSE', 'ICSE', 'X', 'XII'\n",
    "        ]\n",
    "        techskills = open(\"TechSkills.txt\", \"r\").read().lower()\n",
    "        techskills = set(techskills.split())\n",
    "        Skills+=list(techskills)\n",
    "\n",
    "        nontechskills = open(\"Nontech.txt\", \"r\").read().lower()\n",
    "        nontechskills = set(nontechskills.split())\n",
    "        soft_skills=list(nontechskills)\n",
    "\n",
    "        from spacy.lang.en import English\n",
    "\n",
    "\n",
    "        config = {\n",
    "           \"phrase_matcher_attr\":None,\n",
    "           \"validate\": True,\n",
    "          \"overwrite_ents\": False\n",
    "    \n",
    "        }\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "        # move the NER component to the end of the pipeline: remove and then reload from the same source in the new position\n",
    "        nlp.remove_pipe(\"ner\")\n",
    "        # nlp.add_pipe(\"ner\", source=spacy.load(\"en_core_web_sm\"))\n",
    "\n",
    "        # add entity ruler\n",
    "        ruler = nlp.add_pipe(\"entity_ruler\")#, before=\"ner\")\n",
    "\n",
    "        #ruler = nlp.add_pipe(\"entity_ruler\",config=config)\n",
    "        patterns = [{\"label\": \"MOBILE\", \"pattern\": [{\"TEXT\": {\"REGEX\": r\"\\d{10}\"}}]},\n",
    "           {\"label\": \"EMAIL\", \"pattern\": [{\"TEXT\": {\"REGEX\": r'[A-Za-z0-9._%+-]+\\s?@\\s?[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}'}}]},\n",
    "           {\"label\":\"YEAR\",\"pattern\":[{\"TEXT\":{\"REGEX\":r\"^(18|19|20)\\d{2}$\"}}]},\n",
    "            {\"label\":'CGPA',\"pattern\":[{\"TEXT\":{\"REGEX\": r'\\d[.]\\d{2}'}}]}\n",
    "           ]\n",
    "        \n",
    "        indianNames = open(\"allNames.txt\", \"r\").read().lower()\n",
    "        indianNames = set(indianNames.split())\n",
    "\n",
    "        for skill in Skills:\n",
    "            patterns.append({\"label\":\"SKILLS\",\"pattern\":[{\"LOWER\":x.lower()} for x in skill.split()]})\n",
    "        for skill in soft_skills:\n",
    "            patterns.append({\"label\":\"SOFT SKILLS\",\"pattern\":[{\"LOWER\":x.lower()} for x in skill.split()]})\n",
    "    \n",
    "        for project in Projects:\n",
    "            patterns.append({\"label\":\"PROJECT\",\"pattern\":[{\"LOWER\":x.lower()} for x in project.split()]})\n",
    "    \n",
    "        for institute in Institution:\n",
    "            patterns.append({\"label\":\"INSTITUTE\",\"pattern\":[{\"LOWER\":x.lower()} for x in institute.split()]})\n",
    "\n",
    "        for ed in EDUCATION:\n",
    "            patterns.append({\"label\":\"EDUCATION\",\"pattern\":[{\"LOWER\":x.lower()} for x in ed.split()]})\n",
    "    \n",
    "        for exp in Experience:\n",
    "            patterns.append({\"label\":\"EXPERIENCE\",\"pattern\":[{\"LOWER\":exp.lower()}]})\n",
    "    \n",
    "        for name in indianNames:\n",
    "            patterns.append({\"label\":\"PERSON\",\"pattern\":[{\"LOWER\":name.lower()}]})\n",
    "\n",
    "        ruler.add_patterns(patterns)\n",
    "\n",
    "        doc = nlp(self.text)\n",
    "        doc.ents = list(doc.ents)\n",
    "        self.doc_ents = doc.ents\n",
    "        \n",
    "    def get_skills(self):\n",
    "        db=pd.read_csv(\"Skills.csv\")\n",
    "        db.replace(np.nan,\"no\",inplace=True)\n",
    "        SKILLS_DB =[]\n",
    "        for el in db.values:\n",
    "            SKILLS_DB.extend(el)\n",
    "        for i in range(len(SKILLS_DB)):\n",
    "            SKILLS_DB[i]=SKILLS_DB[i].lower()\n",
    "        ConvertedText=self.text\n",
    "        stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "        word_tokens = nltk.tokenize.word_tokenize(ConvertedText)\n",
    "        filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
    "        filtered_tokens = [w for w in word_tokens if w.isalpha()]\n",
    "        bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
    "        found_skills = set()\n",
    "        for token in filtered_tokens:\n",
    "            cols=db.columns\n",
    "            exists=0\n",
    "            for col in range(len(cols)):\n",
    "                n=len(db.values[col])\n",
    "                found=0\n",
    "                matrix=cv.fit_transform([token.lower(),\", \".join(db.values[col]).lower()])\n",
    "                if cosine_similarity(matrix)[1][0]*100 > 15:\n",
    "                    for i in range(n):\n",
    "                        try:\n",
    "                            matrix=cv.fit_transform([token.lower(),db.values[col][i].lower()])\n",
    "                            similarity_matrix=cosine_similarity(matrix)\n",
    "                            if(similarity_matrix[1][0]*100>80):\n",
    "                                found_skills.add(token)\n",
    "                                found=1\n",
    "                                break\n",
    "                        except:\n",
    "                            pass\n",
    "                else:\n",
    "                    pass\n",
    "        for ngram in bigrams_trigrams:\n",
    "            if ngram.lower() in SKILLS_DB:\n",
    "                found_skills.add(ngram)\n",
    "        skills=set()\n",
    "        for skill in found_skills:\n",
    "            if skill not in list(stop_words):\n",
    "                skills.add(skill.lower())\n",
    "        self.Skills=skills\n",
    "        return skills\n",
    "    \n",
    "    def classifier_model(self):\n",
    "        \n",
    "        df = pd.read_csv('UpdatedResumeDataSet.csv')\n",
    "        def cleanResume(resumeText):\n",
    "            resumeText = re.sub('http\\S+\\s*', ' ', resumeText)  # remove URLs\n",
    "            resumeText = re.sub('RT|cc', ' ', resumeText)  # remove RT and cc\n",
    "            resumeText = re.sub('#\\S+', '', resumeText)  # remove hashtags\n",
    "            resumeText = re.sub('@\\S+', '  ', resumeText)  # remove mentions\n",
    "            resumeText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', resumeText)  # remove punctuations\n",
    "            resumeText = re.sub(r'[^\\x00-\\x7f]',r' ', resumeText) \n",
    "            resumeText = re.sub('\\s+', ' ', resumeText)  # remove extra whitespace\n",
    "            return resumeText\n",
    "\n",
    "        df['cleaned'] = df['Resume'].apply(lambda x:cleanResume(x))\n",
    "        label = LabelEncoder()\n",
    "        df['Category'] = label.fit_transform(df['Category'])\n",
    "\n",
    "        le_name_mapping = dict(zip(label.classes_, label.transform(label.classes_)))\n",
    "        le_name_mapping = {v:k for (k,v) in le_name_mapping.items()}\n",
    "\n",
    "        df_text = df['cleaned'].values\n",
    "        target = df['Category'].values\n",
    "        word_vectorizer = TfidfVectorizer(\n",
    "            sublinear_tf=True,\n",
    "            stop_words='english',\n",
    "            max_features=1500)\n",
    "        word_vectorizer.fit(df_text)\n",
    "        WordFeatures = word_vectorizer.transform(df_text)\n",
    "\n",
    "        model = joblib.load('category_classifier.obj')\n",
    "    \n",
    "        WordFeatures = word_vectorizer.transform([self.text])\n",
    "        y_pred = model.predict(WordFeatures.reshape(1,-1)) \n",
    "\n",
    "        self.Category = le_name_mapping[y_pred[0]]\n",
    "    def predict(self):\n",
    "        entities={}\n",
    "        for ent in self.doc_ents:\n",
    "            if ent.label_=='PERSON':\n",
    "                continue\n",
    "            entities.setdefault(ent.label_, set())\n",
    "            entities[ent.label_].add(ent.text)\n",
    "            \n",
    "        Candidate = [ent.text for ent in self.doc_ents if ent.label_=='PERSON'][0]\n",
    "        try:\n",
    "            if self.name==\"Name not found\":\n",
    "                entities['CANDIDATE'] = {Candidate}\n",
    "        except:\n",
    "            entities['CANDIDATE'] = {self.name}\n",
    "        for skill in self.Skills:\n",
    "            entities[\"SKILLS\"].add(skill)\n",
    "        for designation in self.designation:\n",
    "            entities['EXPERIENCE'].add(designation)\n",
    "        entities[\"ORGANIZATIONS\"]=set()\n",
    "        for orga in self.org:\n",
    "            entities['ORGANIZATIONS'].add(orga)\n",
    "        entities['JOB CATEGORY'] = self.Category    \n",
    "        \n",
    "        print(\"ENTITIES :\")\n",
    "        pp = pprint.PrettyPrinter(depth=4)\n",
    "        pp.pprint(entities)\n",
    "        \n",
    "        return entities\n",
    "        \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
